ytest = matrix(y[testIdx[[s]][[1]],], ncol = NN$ny)
}
out_normalize = normalize(xtrain, ytrain, xtest, ytest)
xtrain = out_normalize[[1]]
ytrain = out_normalize[[2]]
xtest = out_normalize[[3]]
mytrain = out_normalize[[7]]
sytrain = out_normalize[[8]]
# Initialize weights & bias
out_initializeWeightBias <- initializeWeightBias(NN)
mp = out_initializeWeightBias[[1]]
Sp = out_initializeWeightBias[[2]]
#mp_1 <- readMat("C:\\Users\\magal\\Documents\\MSc\\M?moire\\TAGI-R\\BostonHousing\\Data from matlab\\mp_11.mat")$mp1
#mp_1 <- matrix(mp_1)
#mp_2 <- readMat("C:\\Users\\magal\\Documents\\MSc\\M?moire\\TAGI-R\\BostonHousing\\Data from matlab\\mp_21.mat")$mp2
#mp_2 <- matrix(mp_2)
#Sp_1 <- readMat("C:\\Users\\magal\\Documents\\MSc\\M?moire\\TAGI-R\\BostonHousing\\Data from matlab\\Sp_11.mat")$Sp1
#Sp_1 <- matrix(Sp_1)
#Sp_2 <- readMat("C:\\Users\\magal\\Documents\\MSc\\M?moire\\TAGI-R\\BostonHousing\\Data from matlab\\Sp_21.mat")$Sp2
#Sp_2 <- matrix(Sp_2)
#mp = matrix(list(), nrow = 2, ncol = 1)
#mp[[1,1]] = mp_1
#mp[[2,1]] = mp_2
#Sp = matrix(list(), nrow = 2, ncol = 1)
#Sp[[1,1]] = Sp_1
#Sp[[2,1]] = Sp_2
# Hyperparameter tuning NOT ENDED/VERIFIED
# if (optHyperparameter == 1){
#   NN$sv = initsv
#   NN$maxEpoch = initmaxEpoch
#   numRuns = NN$numFolds
#   #tuningtime_s = tic()
#   optNumEpochs = NN$optNumEpochs
#   Nepoch4hp = 1
#   svlist = matrix(0, Nepoch4hp, 1)
#   numEpochMaxlist = matrix(0, Nepoch4hp, 1)
#   for (e in 1:Nepoch4hp){
#     print("")
#     sprintf("Opt. Epoch #", e, "/", Nepoch4hp, "|", s)
#     if (e > 1){
#       idxtrain = sample(nrow(ytrain))
#       ytrain = matrix(ytrain[idxtrain,], nrow = length(ytrain[idxtrain,]))
#       xtrain = xtrain[idxtrain,]
#     }
#     out_crossValidation <- crossValidation(NN, NNval, mp, Sp, xtrain, ytrain, optNumEpochs, numRuns)
#     svlist[e,1] = out_crossValidation[[1]]
#     numEpochMaxlist[e,1] = out_crossValidation[[2]]
#   }
# }
# Training
NN$trainMode = 1
#runtime_s = tic()
stop = 0
epoch = 0
while (stop == 0){
if (epoch > 1){
idxtrain = sample(nrow(ytrain))
ytrain = matrix(ytrain[idxtrain,], nrow = length(ytrain[idxtrain,]))
xtrain = xtrain[idxtrain,]
}
epoch = epoch + 1
out_network = network(NN, mp, Sp, xtrain, ytrain)
mp = out_network[[1]]
Sp = out_network[[2]]
if(epoch >= NN$maxEpoch){
stop = 1
}
}
#runtime_e = toc(tuningtime_s)
# Testing
NNtest$sv = NN$sv
out_network = network(NNtest, mp, Sp, xtest, NULL)
ynTest = out_network[[3]]
SynTest = out_network[[4]]
R = matrix(NNtest$sv^2, nrow = nrow(SynTest), ncol = 1)
SynTest = SynTest + R
out_denormalize <- denormalize(ynTest, SynTest, mytrain, sytrain)
ynTest = out_denormalize[[1]]
SynTest = out_denormalize[[2]]
# Evaluation
RMSElist[s] = computeError(ytest, ynTest)
LLlist[s] = loglik(ytest, ynTest, SynTest)
#trainTimelist[s] = runtime_e
print("")
cat(sprintf("Results for Run # %s, RMSE: %s and LL: %s", s, RMSElist[s], LLlist[s]))
new = Sys.time() - old
print(new)
}
metric = list("RMSElist" = RMSElist, "LLlist" = LLlist)
outputs <- list(mp, Sp, metric)
return(outputs)
}
# Main network
network <- function(NN, mp, Sp, x, y){
# Initialization
numObs = nrow(x)
numCovariates = NN$nx
if(!(is.null(NN$errorRateEval))){
NN$errorRateMonitoring = 0
}
zn = matrix(0, numObs, NN$ny)
Szn = matrix(0, numObs, NN$ny)
if (!is.null(NN$errorRateEval)){
if(NN$errorRateEval ==1){
Pn = matrix(0, numObs, NN$numClasses)
P = matrix(0, NN$batchSize, NN$numClasses)
er = matrix(0, numObs, 1)
}
} else {
er = NULL
Pn = NULL
}
# Loop
loop = 0
for (i in seq(from = 1, to = numObs, by = NN$batchSize)){
loop = loop + 1
idxBatch = i:(i + NN$batchSize - 1)
xloop = matrix(t(x[idxBatch,]), nrow = length(idxBatch) * numCovariates, ncol = 1)
# Training
if (NN$trainMode == 1){
yloop = matrix(t(y[idxBatch,]), nrow = length(idxBatch) * NN$ny, ncol = 1)
if (!(is.null(NN$encoderIdx))){
updateIdx = selectIndices(NN$encoderIdx[idxBatch,], NN$batchSize, NN$ny)
} else {
updateIdx = NULL
}
out_feedForward = feedForward(NN, xloop, mp, Sp)
mz = out_feedForward[[1]]
Sz = out_feedForward[[2]]
Czw = out_feedForward[[3]]
Czb = out_feedForward[[4]]
Czz = out_feedForward[[5]]
out_feedBackward = feedBackward(NN, mp, Sp, mz, Sz, Czw, Czb, Czz, yloop, updateIdx)
mp = out_feedBackward[[1]]
Sp = out_feedBackward[[2]]
zn[idxBatch,] = t(matrix(mz[[length(mz)]], nrow = NN$ny, ncol = length(idxBatch)))
Szn[idxBatch,] = t(matrix(Sz[[length(Sz)]], nrow = NN$ny, ncol = length(idxBatch)))
}
# Testing
else {
out_feedForward = feedForward(NN, xloop, mp, Sp)
mz = out_feedForward[[1]]
Sz = out_feedForward[[2]]
zn[idxBatch,] = t(matrix(mz, nrow = NN$ny, ncol = length(idxBatch)))
Szn[idxBatch,] = t(matrix(Sz, nrow = NN$ny, ncol = length(idxBatch)))
}
# Error rate
#if (!is.null(NN$errorRateEval)){
#if(NN$errorRateEval ==1){
#zi = zn[idxBatch,]
#Szi = Szn[idxBatch,]
#for (j in 1:NN$batchSize){
#P[j,] = obs2class(mz, Sz, numClasses)
#}
}
outputs <- list(mp, Sp, zn, Szn)
return(outputs)
}
# Forward network
feedForward <- function(NN, x, mp, Sp){
# Initialization
numLayers = length(NN$nodes)
hiddenLayerActFunIdx = activationFunIndex(NN$hiddenLayerActivation)
# Activation unit
ma = matrix(list(), nrow = numLayers, ncol = 1)
ma[[1,1]] = x
Sa = matrix(list(), nrow = numLayers, ncol = 1)
# Hidden states
mz = matrix(list(), nrow = numLayers, ncol = 1)
Czw = matrix(list(), nrow = numLayers, ncol = 1)
Czb = matrix(list(), nrow = numLayers, ncol = 1)
Czz = matrix(list(), nrow = numLayers, ncol = 1)
Sz = matrix(list(), nrow = numLayers, ncol = 1)
J = matrix(list(), nrow = numLayers, ncol = 1)
# Hidden Layers
for (j in 2:numLayers){
mz[[j,1]] = meanMz(mp[[j-1,1]], ma[[j-1,1]], NN$idxFmwa[j-1,], NN$idxFmwab[[j-1,1]])
# Covariance for z^(j)
Sz[[j,1]] = covarianceSz(mp[[j-1,1]], ma[[j-1,1]], Sp[[j-1,1]], Sa[[j-1,1]],NN$idxFmwa[j-1,], NN$idxFmwab[[j-1,1]])
if (NN$trainMode == 1){
# Covariance between z^(j) and w^(j-1)
out = covarianceCzp(ma[[j-1,1]], Sp[[j-1,1]], NN$idxFCwwa[j-1,], NN$idxFCb[[j-1,1]])
Czb[[j,1]] = out[[1]]
Czw[[j,1]] = out[[2]]
# Covariance between z^(j+1) and z^(j)
if (!(is.null(Sz[[j-1,1]]))){
Czz[[j,1]] = covarianceCzz(mp[[j-1,1]], Sz[[j-1,1]], J[[j-1,1]], NN$idxFCzwa[j-1,])
}
}
# Activation
if (j < numLayers){
out_act = meanA(mz[[2,1]], mz[[2,1]], hiddenLayerActFunIdx)
ma[[2,1]] = out_act[[1]]
J[[j,1]] = out_act[[2]]
Sa[[j,1]] = covarianceSa(J[[j,1]], Sz[[j,1]])
}
}
# Outputs
if (!(NN$outputActivation == "linear")){
ouputLayerActFunIdx = activationFunIndex(NN$outputActivation)
out_feedForward = meanA(mz[[numLayers,1]], mz[[numLayers,1]], ouputLayerActFunIdx)
mz[[numLayers,1]] = out_feedForward[[1]]
J[[numLayers,1]] = out_feedForward[[2]]
Sz[[numLayer,1]] = covarianceSa(J[[numLayers,1]], Sz[[numLayers,1]])
}
if (NN$trainMode == 0){
mz = mz[[numLayers,1]]
Sz = Sz[[numLayers,1]]
}
outputs <- list(mz, Sz, Czw, Czb, Czz)
return(outputs)
}
# Backward network
feedBackward <- function(NN, mp, Sp, mz, Sz, Czw, Czb, Czz, y, udIdx){
# Initialization
numLayers = length(NN$nodes)
mpUd = matrix(list(), nrow = numLayers - 1, ncol = 1)
SpUd = matrix(list(), nrow = numLayers - 1, ncol = 1)
mzUd = matrix(list(), nrow = numLayers - 1, ncol = 1)
SzUd = matrix(list(), nrow = numLayers - 1, ncol = 1)
lHL = numLayers - 1
if (NN$ny == length(NN$sv)){
sv = t(NN$sv)
} else {sv = matrix(NN$sv, nrow = NN$ny, ncol = 1)}
# Update hidden states for the last hidden layer
R = matrix(sv^2, nrow = NN$batchSize, ncol = 1)
Szv = Sz[[lHL + 1]] + R
if (is.null(udIdx)){
out_fowardHiddenStateUpdate = fowardHiddenStateUpdate(mz[[lHL+1]], Sz[[lHL+1]], mz[[lHL+1]], Szv, Sz[[lHL+1]], y)
mzUd[[lHL+1]] = out_fowardHiddenStateUpdate[[1]]
SzUd[[lHL+1]] = out_fowardHiddenStateUpdate[[2]]
}
else{ # TO BE VERIFIED (not used in ToyExample)
mzf = mz[[lHL+1]][udIdx]
Szf = Sz[[lHL+1]][udIdx]
ys = y[udIdx]
Svz = Svz[udIdx]
mzUd[[lHL+1]] = mz[[lHL+1]]
SzUd[[lHL+1]] = Sz[[lHL+1]]
out_fowardHiddenStateUpdate = fowardHiddenStateUpdate(mzf, Szf, mzf, Szv, Szf, ys)
mzUd[[lHL+1]][udIdx] = out_fowardHiddenStateUpdate[[1]]
SzUd[[lHL+1]][udIdx] = out_fowardHiddenStateUpdate[[2]]
}
for (k in (numLayers-1):1){
# Update parameters
Czp = buildCzp(Czw[[k+1]], Czb[[k+1]], NN$nodes[k+1], NN$nodes[k], NN$batchSize)
out_backwardParameterUpdate = backwardParameterUpdate(mp[[k]], Sp[[k]], mz[[k+1]], Sz[[k+1]], SzUd[[k+1]], Czp, mzUd[[k+1]], NN$idxSzpUd[[k]])
mpUd[[k]] = out_backwardParameterUpdate[[1]]
SpUd[[k]] = out_backwardParameterUpdate[[2]]
# Update hidden states
if (k > 1){
Czzloop = buildCzz(Czz[[k+1]], NN$nodes[k+1], NN$nodes[k], NN$batchSize)
out_backwardHiddenStateUpdate = backwardHiddenStateUpdate(mz[[k]], Sz[[k]], mz[[k+1]], Sz[[k+1]], SzUd[[k+1]], Czzloop, mzUd[[k+1]], NN$idxSzzUd[[k]])
mzUd[[k]] = out_backwardHiddenStateUpdate[[1]]
SzUd[[k]] = out_backwardHiddenStateUpdate[[2]]
}
}
outputs <- list(mpUd, SpUd)
return(outputs)
}
meanMz <- function(mp, ma, idxFmwa, idxFmwab){
# mp is the mean of parameters for the current layer
# ma is the mean of activation unit (a) from previous layer
# idxFmwa{1} is the indices for weight w
# idxFmwa{2} is the indices for activation unit a
# idxFmwab is the indices for bias b
# *NOTE*: All indices have been built in the way that we bypass
# the transition step such as F*mwa + F*b
if (nrow(idxFmwa[[1]]) == 1){
idxSum = 2 # Sum by column
} else {
idxSum = 1 # Sum by row
}
mpb = matrix(mp[idxFmwab,], nrow = length(idxFmwab))
mp = matrix(mp[idxFmwa[[1]]], nrow = max(nrow(idxFmwa[[1]]),ncol(idxFmwa[[1]])))
ma = matrix(ma[idxFmwa[[2]]], nrow = max(nrow(idxFmwa[[2]]),ncol(idxFmwa[[2]])))
mWa = matrix(apply(mp * ma, idxSum, sum), nrow = nrow(mpb), ncol = 1)
mz = mWa + mpb
return(mz)
}
covarianceSz <- function(mp, ma, Sp, Sa, idxFSwaF, idxFSwaFb){
# mp is the mean of parameters for the current layer
# ma is the mean of activation unit (a) from previous layer
# Sp is the covariance matrix for parameters p
# Sa is the covariance matrix for a from the previous layer
# idxFSwaF{1} is the indices for weight w
# idxFSwaF{2} is the indices for activation unit a
# idxFSwaFb is the indices for bias
# *NOTE*: All indices have been built in the way that we bypass
# the transition step such as Sa = F*Cwa*F' + F*Cb*F'
if (nrow(idxFSwaF[[1]]) == 1){
idxSum = 2 # Sum by column
} else {
idxSum = 1 # Sum by row
}
Spb = matrix(Sp[idxFSwaFb,], nrow = length(idxFSwaFb))
Sp = matrix(Sp[idxFSwaF[[1]]], nrow = max(nrow(idxFSwaF[[1]]),ncol(idxFSwaF[[1]])))
ma = matrix(ma[idxFSwaF[[2]]], nrow = max(nrow(idxFSwaF[[2]]),ncol(idxFSwaF[[2]])))
if (is.null(Sa)){
Sz = apply(Sp * ma * ma, idxSum, sum)
} else {
mp = matrix(mp[idxFSwaF[[1]]], nrow = max(nrow(idxFSwaF[[1]]),ncol(idxFSwaF[[1]])))
Sa = matrix(Sa[idxFSwaF[[2]]], nrow = max(nrow(idxFSwaF[[2]]),ncol(idxFSwaF[[2]])))
Sz = apply(Sp * Sa + Sp * ma * ma + Sa * mp * mp, idxSum, sum)
}
Sz = Sz + Spb
return(Sz)
}
covarianceCzp <- function(ma, Sp, idxFCwwa, idxFCb) {
# ma is the mean of activation unit (a) from previous layer
# Sp is the covariance matrix for parameters p
# idxFCwwa{1} is the indices for weight w
# idxFCwwa{2} is the indices for weight action unit a
# idxFcb is the indices for bias b
# *NOTE*: All indices have been built in the way that we bypass
# the transition step such as Cpa = F*Cpwa + F*Cb
Czb = matrix(Sp[idxFCb,], nrow = length(idxFCb))
Sp = matrix(Sp[idxFCwwa[[1]]], nrow = nrow(idxFCwwa[[1]]))
ma = matrix(ma[idxFCwwa[[2]]], nrow = nrow(idxFCwwa[[2]]))
Czw = Sp * ma
outputs <- list(Czb, Czw)
return(outputs)
}
covarianceCzz <- function(mp, Sz, J, idxCawa) {
Sz = matrix(Sz[idxCawa[[2]]], nrow = nrow(idxCawa[[2]]))
mp = matrix(mp[idxCawa[[1]]], nrow = nrow(idxCawa[[1]]))
J = matrix(J[idxCawa[[2]]], nrow = nrow(idxCawa[[2]]))
Czz = J * Sz * mp
return(Czz)
}
# Update Step
backwardParameterUpdate <- function(mp, Sp, mzF, SzF, SzB, Czp, mzB, idx){
dz = mzB - mzF
dz = matrix(dz[idx,], nrow = length(idx))
dS = SzB - SzF
dS  = matrix(dS[idx,], nrow = length(idx))
SzF = 1 / SzF
SzF = matrix(SzF[idx,], nrow = length(idx))
J   = Czp * SzF
# Mean
mpUd = mp + rowSums(J * dz)
# Covariance
SpUd = Sp + rowSums(J * dS * J)
outputs <- list(mpUd, SpUd)
return(outputs)
}
backwardHiddenStateUpdate <- function(mz, Sz, mzF, SzF, SzB, Czz, mzB, idx){
dz = mzB - mzF
dz = matrix(dz[idx,], nrow = length(idx))
dS = SzB - SzF
dS  = matrix(dS[idx,], nrow = length(idx))
SzF = 1 / SzF
SzF = matrix(SzF[idx,], nrow = length(idx))
J   = Czz * SzF
# Mean
mzUd = mz + rowSums(J * dz)
# Covariance
SzUd = Sz + rowSums(J * dS * J)
outputs <- list(mzUd, SzUd)
return(outputs)
}
fowardHiddenStateUpdate <- function(mz, Sz, mzF, SzF, Cyz, y){
dz = y - mzF
SzF = 1 / SzF
K = Cyz * SzF
# Mean
mzUd = mz + K * dz
# Covariance
SzUd = Sz - K * Cyz
#Outputs
out <- list(mzUd, SzUd)
return(out)
}
# Build the matrix Czp, Czz for the update step
buildCzp <- function(Czw, Czb, currentHiddenUnit, prevHiddenUnit, batchSize){
Czp = rbind(Czb, Czw)
Czp = t(matrix(Czp, nrow = batchSize, ncol = currentHiddenUnit*prevHiddenUnit + currentHiddenUnit))
return(Czp)
}
buildCzz <- function(Czz, currentHiddenUnit, prevHiddenUnit, batchSize){
Czz = t(matrix(Czz, nrow = currentHiddenUnit, ncol = prevHiddenUnit*batchSize))
return(Czz)
}
# Initialization weights and bias
initializeWeightBias <- function(NN){
# Initialization
NN$dropWeight = NULL
NN <- c(NN, dropWeight = 0)
nodes = NN$nodes
numLayers = length(nodes)
idxw = NN$idxw
idxb = NN$idxb
factor4Bp = NN$factor4Bp
factor4Wp = NN$factor4Wp
mp = matrix(list(), nrow = numLayers - 1, ncol = 1)
Sp = matrix(list(), nrow = numLayers - 1, ncol = 1)
for (j in 2:numLayers){
# Bias variance
Sbwloop_1 = factor4Bp[j-1] * matrix(1L, nrow = 1, ncol = length(idxb[[j-1, 1]]))
# Bias mean
bwloop_1 = rnorm(ncol(Sbwloop_1)) * sqrt(Sbwloop_1)
# Weight variance
if (NN$hiddenLayerActivation == "relu" || NN$hiddenLayerActivation == "softplus" || NN$hiddenLayerActivation == "sigm"){
Sbwloop_2 = factor4Wp[j-1] * (1/nodes[j-1]) * matrix(1L, nrow = 1, ncol = length(idxw[[j-1, 1]]))
} else {
Sbwloop_2 = factor4Wp[j-1] * (2/nodes[j-1] + nodes[j]) * matrix(1L, nrow = 1, ncol = length(idxw[[j-1, 1]]))
}
# Weight mean
if (NN$dropWeight == 0){
bwloop_2 = rnorm(ncol(Sbwloop_2)) * sqrt(Sbwloop_2)
} else {
bwloop_2 = matrix(0, nrow = 1, ncol = length(idxw[[j-1, 1]]))
}
mp[[j-1, 1]] = t(cbind(bwloop_1, bwloop_2))
Sp[[j-1, 1]] = t(cbind(Sbwloop_1, Sbwloop_2))
}
outputs <- list(mp, Sp)
return(outputs)
}
nobs <- nrow(MedicalCost)
ncvr <- 9
# Input features
x <- MedicalCost[,1:ncvr]
# Output targets
y <- matrix(MedicalCost[,10], ncol = 1)
nx <- ncol(x)
ny <- ncol(y)
########################### Neural Network properties ##########################
set.seed(100)
NN <- list(
"nx" = nx, # Number of input covariates
"ny" = ny, # Number of output responses
"batchSizeList" = c(1, 1, 1), # Batch size [train, val, test]
"nodes" = c(nx, 100, ny), # Number of nodes for each layer
"sx" = NULL, # Input standard deviation
"sv" = 0.32 * matrix(1L, nrow = 1, ncol = ny), # Observations standard deviation
"maxEpoch" = 40, # maximal number of learnign epoch
"hiddenLayerActivation" = "relu", # Activation function for hidden layer {'tanh','sigm','cdf','relu','softplus'}
"outputActivation" = "linear", # Activation function for hidden layer {'linear', 'tanh','sigm','cdf','relu'}
# Optimize hyperparameters?
"optsv" = 0, # Observation noise sv 1: yes; 0: no
"optNumEpochs" = 0, # Number of epochs 1: yes; 0: no
"numEpochs" = 0, # Number of searching epochs
"numFolds" = 0, # Number of Folds for cross-validation
"ratio" = 0.8, # Ratio between training set and validation set
"numSplits" = 20, # Number of splits
"task" = "regression" # Task regression or classification
)
NN$factor4Bp = 0.01 * matrix(1L, nrow = 1, ncol = length(NN$nodes) - 1) # Factor for initializing bias
NN$factor4Wp = 0.25 * matrix(c(1/NN$nodes[1],1/NN$nodes[2]), nrow = 1, ncol = 2) # Factor for initializing weights
trainIdx <- NULL
testIdx <- NULL
out_regression <- regression(NN, x, y, trainIdx, testIdx)
mp = out_regression[[1]]
Sp = out_regression[[2]]
metric = out_regression[[3]]
time = out_regression[[4]]
print("Final Results")
cat(sprintf("Average RMSE: %s +- %s", mean(metric$RMSElist), std(metric$RMSElist)))
print("")
cat(sprintf("Average LL: %s +- %s", mean(metric$LLlist), std(metric$LLlist)))
library(matlab)
library(mvtnorm) # graphs
library(R.matlab) # indices function
library(tictoc) # tagi functions
nobs <- nrow(MedicalCost)
ncvr <- 9
# Input features
x <- MedicalCost[,1:ncvr]
# Output targets
y <- matrix(MedicalCost[,10], ncol = 1)
nx <- ncol(x)
ny <- ncol(y)
########################### Neural Network properties ##########################
set.seed(100)
NN <- list(
"nx" = nx, # Number of input covariates
"ny" = ny, # Number of output responses
"batchSizeList" = c(1, 1, 1), # Batch size [train, val, test]
"nodes" = c(nx, 100, ny), # Number of nodes for each layer
"sx" = NULL, # Input standard deviation
"sv" = 0.32 * matrix(1L, nrow = 1, ncol = ny), # Observations standard deviation
"maxEpoch" = 40, # maximal number of learnign epoch
"hiddenLayerActivation" = "relu", # Activation function for hidden layer {'tanh','sigm','cdf','relu','softplus'}
"outputActivation" = "linear", # Activation function for hidden layer {'linear', 'tanh','sigm','cdf','relu'}
# Optimize hyperparameters?
"optsv" = 0, # Observation noise sv 1: yes; 0: no
"optNumEpochs" = 0, # Number of epochs 1: yes; 0: no
"numEpochs" = 0, # Number of searching epochs
"numFolds" = 0, # Number of Folds for cross-validation
"ratio" = 0.8, # Ratio between training set and validation set
"numSplits" = 20, # Number of splits
"task" = "regression" # Task regression or classification
)
NN$factor4Bp = 0.01 * matrix(1L, nrow = 1, ncol = length(NN$nodes) - 1) # Factor for initializing bias
NN$factor4Wp = 0.25 * matrix(c(1/NN$nodes[1],1/NN$nodes[2]), nrow = 1, ncol = 2) # Factor for initializing weights
trainIdx <- NULL
testIdx <- NULL
out_regression <- regression(NN, x, y, trainIdx, testIdx)
mp = out_regression[[1]]
Sp = out_regression[[2]]
metric = out_regression[[3]]
time = out_regression[[4]]
print("Final Results")
cat(sprintf("Average RMSE: %s +- %s", mean(metric$RMSElist), std(metric$RMSElist)))
print("")
cat(sprintf("Average LL: %s +- %s", mean(metric$LLlist), std(metric$LLlist)))
library(devtools)
library(roxygen2)
library(tagi)
check()
file.exists("~/.ssh/id_rsa.pub")
